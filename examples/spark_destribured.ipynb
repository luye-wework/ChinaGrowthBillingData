{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark destributed mode\n",
    "this notebook will show you the basic steps needed to be done in order to be able to use spark in distributed mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We highly recommend that you work with jupyert store magic and place all sensative credentials in a different file.\n",
    "the `%store -r` command below will retrive any variable being place in jupyterlab's cache.\n",
    "in this example the `redshift_user` and `redshift_password` variables were declared in `mysecretstore.ipynb` and retrived by `%store -r` command.\n",
    "see more information [here](https://connect.weworkers.io/pages/editpage.action?pageId=147130442)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import socket\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from wework_utils.spark import getWek8sConf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All distributed mode configuration is hidden in the `getWek8sConf()`. \n",
    "if you want to see the explicit set of configuration please see [here](https://github.com/WeConnect/xprmt/blob/master/utils/wework_utils/spark.py).\n",
    "All additional  configuratinon should be supplied by you (see below an example for declaring the number of executors)\n",
    "For a complete list of spark configuration see [here](https://spark.apache.org/docs/latest/configuration.html#available-properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = SparkSession.builder.config(conf=getWek8sConf()) \\\n",
    ".config(\"spark.executor.instances\",\"1\").getOrCreate() \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example on how to work with saprk redshift connector. \n",
    "you should provide the redshift username/password and the table and the s3 uri in which data from redshift will be unload to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = session.read \\\n",
    "    .format(\"com.databricks.spark.redshift\") \\\n",
    "    .option(\"url\", f\"jdbc:redshift://redshift-production.weworkers.io:5439/analyticdb?user={redshift_user}&password={redshift_password}\") \\\n",
    "    .option(\"dbtable\", \"mytable\") \\\n",
    "    .option(\"tempdir\", \"s3a://mybacket/path/to/folder\") \\\n",
    "    .option(\"tempformat\",\"CSV\") \\\n",
    "    .option(\"forward_spark_s3_credentials\",\"true\") \\\n",
    "    .load()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
